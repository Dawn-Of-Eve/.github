# Dawn Of Eve: letting Adam Optimiser rest in peace

Optimisers are the hammers that shape the models which perform miracles. They are the fundamental to Machine Learning and their research essential to building artificial intelligence. Advancement in optimisers benefits all and harms none. So why has the optimiser community had little effect on the overall progress of AI research in the past decade? why are we still where we were in 2014?

Adam optimiser (Kingma et al., 2014) has dominated in the arena of optimisers for neural networks, and no other algorithm has been able to come close to its wide applicability, speed, reliability and overall popularity. Many have tried to dethrone it but none have prevailed. Till now...

We at Dawn of Eve plan to build tools and resources to help dethrone Adam once and for all, and we plan to do so with the following repositories:

1. **Awesome Optimisers**: A repository built with love and care for Optimiser Researchers to speed up going from beginner to Godly optimally. Literature review should not hold you back anymore.
2. **Nadir**: A python library built on top of PyTorch for faster, bleeding edge, general purpose optimisers that can get you to the point of ***Nadir*** the fastest.   
3. ***More coming soon, sit tight***


## Join us! we are seeking collaborators

Hey! if you are interested in the same, why don't you help us out?

There are a few ways you can help us:
1. Sponser us! We try to work hard to get you the best of optimisers for no cost at all, but a coffee or a toffee would be quite motivating :smile:
2. Make contributions to our repositories! All contributors are welcome and we would be delighted to have people help us. 
3. Contact the maintainer to join the team, attend meetings and help build the Organisation from ground up with the Team Members. It is a commitment but we strongly believe we make the world a better place by optimising it (and having a lil' bit of fun along the way)


<!-- 
## Research Problems/Motivation

Most people in the NLProc community directly utilize Adam Optimizers as a design choice without going through extensive search over optimizers, simply because of its robustness out-of-the-box. While helpful in abstrating out this part of the pipeline, it might be hurting performance more than one would realise, since the type of optimizer can determine the quality of model quiet drastically. (not all optimizers are the same)

Research Problems this project is trying to tackle:

* "Where and how has Adam been utilized for large language modelling?"
    * There must be a reason why Adam is so dominant for large language modelling, and understanding that is the first step to understading the alternative options to Adam. 

* "What alternative options are there to Adam? Are they competitive?"
    * There are a number of considerations to make while choosing the optimizer for LLM training, like convergence speed, memory overload, training speed and more. For alternatives to be truely competitive, they should present significiant improvements in one or more of these factors.
    * Another important factor to understand whether Adam should be replaced for LLM training is that "can you live without the alternative?". If an alternative is insignificant in its benefit as compared to Adam, utilizing it would not make sense because of the deeprooted-ness of the Adam in the community and ease of use from being readily available in frameworks.

* "I see the claims of the alternatives but how well do they actually perform in practice?"
    * Without extensive experimentation and emperical (hard and cold) evidence to back the claims of improvement over Adam, nothing really matters. That's the beauty of the research community based on peer-review. 
    * Tragically, one of the reasons why most of the recent work on convex optimisation has not made it to the industry (at least for LLMing) is the lack of proper testing on language modelling objectives, used with transformer-like models. Most papers, if at all, test LLMing on LSTMs, which learn differently than Transformers and might have different results. 
 -->

<!-- ## Citation -->




<!-- 
If you wish to cite this work, please use the following bibtex:
```bibtex
``` -->

<!-- For a list of citations regarding the papers used to make this repository, please refer to [citations.md](citations.md). If any citation is missing please inform the repository maintainer to get it included.  -->

## Misc.

Here's a poem written for the demise of Adam:
> Adam is Old  
> Adam is Tired  
> Adam has Back-Pain  
> Adam wants to Retire  
>                       ~@bhavnicksm, 2022

Good thing @bhavnicksm is better at writting PyTorch code than Poems :smirk:
